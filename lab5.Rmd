---
title: "Lab 5: The Sound of Gunfire, Off in the Distance"
author: "Claire Jellison"
date: "10/12/2019"
output: pdf_document
---

```{r}
library(ggplot2)
library(dplyr)
```


### Chapter 4 Exercises

```{r}
war <- read.csv("http://www.stat.cmu.edu/~cshalizi/uADA/15/hw/06/ch.csv", row.names = 1)
```

1)Estimate: Fit a logistic regression model for the start of civil war on all other variables except country and year (yes, this makes some questionable assumptions about independent observations); include a quadratic term for exports. Report the coefficients and their standard errors, together with R’s p-values. Which ones are found to be significant at the 5% level?

```{r}
head(war)
exports2 <- war$exports^2
logmodel <- glm(start ~ exports + exports2 + schooling + growth + peace + concentration + lnpop +fractionalization + dominance, data = war, family = binomial)
summary(logmodel)
```
Exports, exports^2, schooling, growth, peace, concentration, lnpop and fractionalization are all significant at the 95% confidence level. 


2)Interpretation: All parts of this question refer to the logistic regression model you just fit.

a)What is the model’s predicted probability for a civil war in India in the period beginning 1975? What probability would it predict for a country just like India in 1975, except that its male secondary school enrollment rate was 30 points higher? What probability would it predict for a country just like India in 1975, except that the ratio of commodity exports to GDP was 0.1 higher?

```{r}
logmodel.probs = predict(logmodel, type = "response")

india1975 <- filter(war, country == "India", year =="1975")
india1975$exports2 = india1975$exports^2

indiapred <- predict(logmodel, india1975, type = "response")
indiapred

likeindia <- india1975 %>% 
  mutate(schooling = schooling + 30)

likeindiapred <- predict(logmodel, likeindia , type = "response")
likeindiapred

likeindia2 <- india1975 %>% 
  mutate(exports = exports + .1)
likeindia2$exports2 = likeindia2$exports^2

likeindiapred2 <- predict(logmodel, likeindia2 , type = "response")
likeindiapred2

```

The predicted probability for a civil war in India in the period begining in 1975 was 35%. If the only difference was a school enrollmentrat that wa 30 points higher then the predicted probability of civil war would go down to 17%. With the higher ratio of commodity exports the predicted probability went up to around 70%.


b)What is the model’s predicted probability for a civil war in Nigeria in the period beginning 1965? What probability would it predict for a country just like Nigeria in 1965, except that its male secondary school enrollment rate was 30 points higher? What probability would it predict for a country just like Nigeria in 1965, except that the ratio of commodity exports to GDP was 0.1 higher?

```{r}
nigeria1965 <- filter(war, country == "Nigeria", year =="1965")
nigeria1965$exports2 = nigeria1965$exports^2
nigeria1965

nigeriapred <- predict(logmodel, nigeria1965, type = "response")
nigeriapred


likenigeria <- nigeria1965 %>% 
  mutate(schooling = schooling + 30)

likenigeriapred <- predict(logmodel, likenigeria , type = "response")
likenigeriapred

likenigeria2 <- nigeria1965 %>% 
  mutate(exports = exports + .1)
likenigeria2$exports2 = likenigeria2$exports^2

likenigeriapred2 <- predict(logmodel, likenigeria2 , type = "response")
likenigeriapred2

```

The predicted probability for a civil war in Nigeria in the period begining in 1965 was 17%. With the higher schooling rate the probability drops to around 7.4%. Finally with the greater exports to GDP ration the predicted probability goes to around 33%. 


c)In the parts above, you changed the same predictor variables by the same amounts. If you did your calculations properly, the changes in predicted probabilities are not equal. Explain why not. (The reasons may or may not be the same for the two variables.)

Unlike a linear model, the slope is not constant on a logistic model for any given value of x. 


3)Confusion: Logistic regression predicts a probability of civil war for each country and period. Suppose we want to make a definite prediction of civil war or not, that is, to classify each data point. The probability of misclassification is minimized by predicting war if the probability is ≥ 0.5, and peace otherwise.



a)Build a 2 × 2 confusion matrix (a.k.a. “classification table” or “contigency table”) which counts: the number of outbreaks of civil war correctly predicted by the logistic regression; the number of civil wars not predicted by the model; the number of false predictions of civil wars; and the number of correctly predicted absences of civil wars. (Note that some entries in the table may be zero.)

b)What fraction of the logistic regression’s predictions are incorrect, i.e. what is the misclassification rate? (Note that this is if anything too kind to the model, since it’s looking at predictions to the same training data set).

c)Consider a foolish (?) pundit who always predicts “no war”. What fraction of the pundit’s predictions are correct on the whole data set? What fraction are correct on data points where the logistic regression model also makes a prediction?



4)Comparison: Since this is a classification problem with only two classes, we can compare Logistic Regression right along side Discriminant Analysis.

a)Fit an LDA model using the same predictors that you used for your logistic regression model. What is the training misclassification rate?

b)Fit a QDA model using the very same predictors. What is the training misclassification rate?
How does the prediction accuracy of the three models compare? Why do you think this is?

c)How does the prediction accuracy of the three models compare? Why do you think this is?

Challenge problem: Using the code available from class slides, construct an ROC curve for your logistic regression model. For an extra challenge, plot the ROC curves of all three models on the same plot.



### Exercise 4

When the number of features p is large, there tends to be a deteri- oration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test ob- servation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse.

a) If we are only useing the 10% of the range of X closest to that test observation, then we will only be using on average 1/10 of the available observations. 

b) Now we are only using 1/100 of the available observations because the observations must be in the ranges for both of the identified features. We can imagine all the observations as being scattered in a box [0,1] X [0,1] with area = 1. Then for the specified ranges the area inside the box of observations we can use is $.1 *.1 = .01$

c)With p = 100, we now can only use $.1^{100}$ of our available observations. 

d) From the above, we see that KNN may not work well when the number of observations is small compared to the number of identified features. For any given test observation, we may end up with only a few training observations nearby to predict it's value. This will result in a high amount of variability and a scenario wherein outliers can have a very large influence due to the scarcity of observations. In order to have sufficient observations one may have to extend the range they use to predict however this could result in more bias as the range increases. 

e) For p = 1 the hypercube is a line segement with length 0.1, for p = 2 the hypercube is a square with 
length $\sqrt{.1}$, for p = 100 the hypercube has side length $\sqrt[100]{.1}$. This is because $\sqrt{.1}^{2} = .1$ meaning that it includes on average 10% of the training observations given that they lie in [0,1] along both axes. The same logic applies to p = 100. We see that the side length of the cube can decrease with added p, so long as the volume (or p dimensional equivalent) has 1/10 of the total observations on average. 



### Exercise 6 

Suppose we collect data for a group of students in a statistics class with variables X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, $\beta^{0} = -6, \beta^{1} = 0.05, \beta^{2} = 1$ 

(a) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.

Our equation is  $P(Y=1) = \frac{e^{\beta^{0} + \beta^{1}(X_1) + \beta^{2}(X_2)}}{1 + e^{\beta^{0} + \beta^{1}(X_1) + \beta^{2}(X_2)}}$. Plugging in our values, we get 

```{r}
p = exp(-6 + .05*40 + 3.5) / (1 + exp(-6 + .05*40 + 3.5))
p
```
This indicates that there is around a 38% chance that the student will recieve an A according to our model. 

(b) How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?

Rearraging the formula above we can solve for $X_2$. Below, is the plot 

```{r}

probofA <- function(x) {exp(-6 + .05*x + 3.5) / (1 + exp(-6 + .05*x+ 3.5))}
plot(probofA, 0, 150)
x_vector <- seq(0,100, by = 1)
y_vector = probofA(x_vector)
mat <- cbind(x_vector,y_vector)
data <- data.frame("X" = x_vector, "Y" = y_vector)
data$Y


```

The student with the 3.5 gpa would need to study 50 hrs according to the model to a have a 50% chance of getting an A in the class. 


### Exercise 7 

Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on X, last year’s percent profit. We examine a large number of companies and discover that the mean value of X for companies that issued a dividend was mean X̄ = 10, while the mean for those that didn’t was X ̄ = 0. In addition, the variance of X for these two sets of companies was $\sigma^{2}=36$. Finally, 80 % of companies issued dividends. Assuming that X follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was X = 4 last year.


look at pg 139 

Using Bayes' theorem we can write that, 

$Pr(Y = Yes \mid X= 4) = \frac{\pi_y f_y(x)}{\pi_y f_k(y) + \pi_n f_k(n)} $

We know that $f_(x) = \frac{e^{-(x - u)^{2}/2\sigma^{2}}}{\sqrt{2\pi\sigma^{2}}}$. 

```{r}
normaldist <- function(x, u, o){ (exp(-(x - u)^2/(2*o)))/ sqrt(2*pi*o)}
fy = normaldist(4, 10 , 36)
fn = normaldist(4, 0 ,36)
piy = .8
pin = .2
prob = piy*fy /(pin*fn + piy*fy)
prob
```
Therefore, given that the percentage profit was four, the likelihood of getting a dividend is around 75%. 

Hint: Recall that the density function for a normal random variable is f(x) = √ 1 e−(x−μ)2/2σ2 . You will need to use Bayes’ theorem.



