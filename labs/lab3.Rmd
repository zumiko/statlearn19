---
title: "Regression Competition"
author: "Claire Jellison"
date: "9/25/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
d <- read.csv("http://andrewpbray.github.io/data/crime-train.csv")
#summary(d)
library(ggplot2)
library(glmnet)
library(leaps)
```


### Fitting the model 

```{r}

group_D_fit <- function(training_data) {
  group_D_process(training_data)
  
lm(data = training_data, ViolentCrimesPerPop ~  
            factor(state) 
          + racePctWhite 
          + pctUrban  
          + PctEmploy 
          + MalePctDivorce 
          + MalePctDivorce2 
          + PctKids2Par 
          + PctKids2Par2  
          + PctWorkMom 
          + PctPersDenseHous 
          + NumStreet 
          + PctVacantBoarded 
          + PctImmigRec8 
          + PctImmigRec82 
          + PctIlleg 
          + PctHousOccup 
          + PctWorkMom*MalePctDivorce 
          + pctUrban*racePctWhite 
          + PctEmploy*racePctWhite
          + pctUrban*PctHousOccup 
          + PctEmploy*pctUrban 
          + PctIlleg*PctEmploy 
          + PctImmigRec8*PctVacantBoarded 
          + PctNotHSGrad 
          + PctLess9thGrade 
          + NumInShelters
          + PctEmploy*pctUrban 
          + PctIlleg*PctEmploy 
          + PctImmigRec8*PctVacantBoarded
          + PctNotHSGrad 
          + PctLess9thGrade 
          + NumInShelters)


}
```

```{r}
summary(m11)
```


##  Computing MSE

```{r}
group_D_MSE <- function(model, data){
  n <- nrow(data)
  ys <- data$ViolentCrimesPerPop 
  y_hats <- predict(model, data)
  residuals <- y_hats - ys
  MSE <- sum(residuals^2)/n
  MSE
}
```

## Process 

```{r}
group_D_process() <- function(d){
  d$MalePctDivorce2 <- d$MalePctDivorce^2
  d$PctKids2Par2 <- d$PctKids2Par^2
  d$PctImmigRec82 <- d$PctImmigRec8^2
  d
}
```


## Automated Fit Forward

Here, we chose to minimize the BIC. 
```{r}
group_D_automated_fit <- function(data){ 
  install.packages("leaps")
  library(leaps)
  
  data <- data %>%
    select(population:MedRent, ViolentCrimesPerPop)
  
  forward <- regsubsets(ViolentCrimesPerPop ~ ., data = data, 
                nvmax = 25, method = "forward")
  sum.fwd <- summary(forward)
  i <- which.min(sum.fwd$bic)
  coefs <- coef(forward, i) 
  predictors <- names(coefs)[-1]
  f <- as.formula(
    paste("ViolentCrimesPerPop", 
          paste(predictors, collapse = " + "), 
          sep = " ~ "))
  
  lm(f, data = data)

  
}

```


Below, we examine the MSE. 
```{r}

m1 <- group_D_automated_fit(d)
group_D_MSE(m1, d)


```


## Automated Fit Backward

Again, we chose the model with the lowest BIC. 
```{r}

group_D_automated_fit_back <- function(data){ 
  install.packages("leaps")
  library(leaps)
  
  data <- data %>%
    select(population:MedRent, ViolentCrimesPerPop)
  
  backward <- regsubsets(ViolentCrimesPerPop ~ ., data = data, 
                nvmax = 25, method = "backward")

  sum.bwd <- summary(backward)
  
  i <- which.min(sum.bwd$bic)
  coefs <- coef(backward, i) 
  predictors <- names(coefs)[-1]
  fb <- as.formula(
    paste("ViolentCrimesPerPop", 
          paste(predictors, collapse = " + "), 
          sep = " ~ "))
  
  lm(fb, data = data)
  #summary(b)
  
}

```


Checking the MSE for the backwards one, we see that it is slightly lower that the forward one and therefore it will be our preferred model. 
```{r}

m2 <- group_D_automated_fit_back(d)
summary(m2)
group_D_MSE(m2, d)

```



Looking at how the adjusted $$R^{2}$$ and BIC changes with the number of predictors. We see that while the adjusted R^2 keeps creeping upwards with more predictors the BIC is minimized around 7 or 8 predictors. 
```{r}
dsmall <- d %>%
    select(population:MedRent, ViolentCrimesPerPop)

backward <- regsubsets(ViolentCrimesPerPop ~ ., data = dsmall, nvmax = 25, method = "backward")
forward <- regsubsets(ViolentCrimesPerPop ~ ., data = dsmall, nvmax = 25, method = "forward")
b <- summary(backward)
plot(b$bic, type = "l", ylab = "BIC")
plot(b$adjr2, type = "l", ylab = "Adjusted R^2")


f <- summary(forward)
plot(f$bic, type = "l", ylab = "BIC")
plot(f$adjr2, type = "l", ylab = "Adjusted R^2")

```


```{r}
forward <- regsubsets(ViolentCrimesPerPop ~ ., data = dsmall, 
                nvmax = 25, method = "forward")
attributes(forward)

```



Started by examining the correlation between the predictor variables and ViolentCrimesPerPop, to get an idea of which ones could be good predictors. 
```{r}
#install.packages("Hmisc")
#library("Hmisc")
bool <- sapply(d, is.numeric)
num_only <- d[,bool]

matrix <- cor(num_only)
matrix <- matrix[,"ViolentCrimesPerPop"]
matrix
```



Used a lasso regression approach to decide on the best predictors to include in the model. 
```{r}
#set.seed(489)
#x_vars <- model.matrix(ViolentCrimesPerPop~. , num_only)[,-1]
#y_var <- num_only$ViolentCrimesPerPop
#lambda_seq <- 10^seq(2, -2, by = -.1)
#train = sample(1:nrow(x_vars), nrow(x_vars)/2)
#test = (-train)
#ytest = y[test]
#cv_output <- cv.glmnet(x_vars[train,], y_var[train],
                       #alpha = 1, lambda = lambda_seq)
#best_lam <- cv_output$lambda.min
#lasso.mod <- glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = lambda)
#lasso.pred <- predict(lasso.mod, s = bestlam, newx = x_vars[test,])
#x <- cor(num_only)
#lasso_best <- glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = best_lam)
#pred <- predict(lasso_best, s = best_lam, newx = x_vars[test,])
#final <- cbind(y_var[test], pred)
#head(final)
```

Checked for non linear relationships using residual and normal plots with the chosen variables and added a couple squared terms where it appeared appropriate. 
```{r}
MalePctDivorce2 <- (d$MalePctDivorce)^2
PctKids2Par2 <-(d$PctKids2Par)^2
PctImmigRec82 <- (d$PctImmigRec8)^2
#pctdensehouse2 <- (d$PctPersDenseHous)^2
#m2 <- lm(data = d, ViolentCrimesPerPop ~ state + racePctWhite + pctUrban + PctUnemployed + PctEmploy + MalePctDivorce + PctKids2Par + PctWorkMom + PctPersDenseHous + NumStreet + PctVacantBoarded + PctImmigRec8 + PctIlleg + PctHousOccup + LemasPctOfficDrugUn) 
#m3 <- lm(data = d, ViolentCrimesPerPop ~  state + racePctWhite + pctUrban + PctUnemployed + MalePctDivorce + maledivorce2 + PctKids2Par + PctWorkMom + PctPersDenseHous + NumStreet + PctVacantBoarded + PctImmigRec8 + PctIlleg + PctHousOccup + LemasPctOfficDrugUn) 
#m4 <- lm(data = d, ViolentCrimesPerPop ~  state + racePctWhite + pctUrban + PctUnemployed + MalePctDivorce + maledivorce2 + PctKids2Par + pctkids2par2 + PctWorkMom + PctPersDenseHous + NumStreet + PctVacantBoarded + PctImmigRec8 + PctIlleg + PctHousOccup + LemasPctOfficDrugUn) 
#ggplot(d, (aes(x = LemasPctOfficDrugUn , y = ViolentCrimesPerPop))) + geom_point(position = "jitter")

```

Added some interaction terms that impoved the $R^{2}$, adjusted $R^{2}$ and MSE. Looked at plots of residuals. Residuals vs fitted values appeared pretty good despite a slight dip. 
```{r}
m7 <- lm(data = d, ViolentCrimesPerPop ~  state + racePctWhite + pctUrban  + PctEmploy + MalePctDivorce + MalePctDivorce2 + PctKids2Par + PctKids2Par2  + PctWorkMom + PctPersDenseHous  + NumStreet + PctVacantBoarded + PctImmigRec8 + PctImmigRec82 + PctIlleg +  PctHousOccup + LemasPctOfficDrugUn + PctWorkMom*MalePctDivorce + pctUrban*racePctWhite + racePctWhite*PctIlleg*PctWorkMom + PctEmploy*racePctWhite + pctUrban*PctHousOccup + PctEmploy*pctUrban + PctIlleg*PctEmploy + PctImmigRec8*PctVacantBoarded + PctNotHSGrad + PctLess9thGrade + NumInShelters) 

plot(m7, which=1, col=c("blue")) 
plot(m7, which=2, col=c("red")) 
#summary(m7)
#group_D_MSE(m7, d)
```


Added back some variables that were not included in the lasso, but improved the model. Noticed some poor residuals at the more extreme values.
```{r}

m11 <- lm(data = d, ViolentCrimesPerPop ~  factor(state) + racePctWhite + pctUrban  + PctEmploy + MalePctDivorce + MalePctDivorce^2 + PctKids2Par + PctKids2Par^2  + PctWorkMom + PctPersDenseHous  + NumStreet + PctVacantBoarded + PctImmigRec8 + PctImmigRec8^2 + PctIlleg +  PctHousOccup + PctWorkMom*MalePctDivorce + pctUrban*racePctWhite + PctEmploy*racePctWhite + pctUrban*PctHousOccup + PctEmploy*pctUrban + PctIlleg*PctEmploy + PctImmigRec8*PctVacantBoarded + PctNotHSGrad + PctLess9thGrade + NumInShelters+PctEmploy*pctUrban + PctIlleg*PctEmploy + PctImmigRec8*PctVacantBoarded+PctNotHSGrad + PctLess9thGrade + NumInShelters)
summary(m11)

plot(m11, which=2, col=c("red")) 
```

Assessed the final models looking by two different criterion statistics. 
```{r}
AIC(m7)
BIC(m7)
AIC(m11)
BIC(m11)
```

