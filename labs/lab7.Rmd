---
title: "Lab 7: When a guest arrives they will count how many sides it has on"
author: "Claire Jellison"
date: "11/8/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(randomForest)
```


```{r}
set.seed(75)
n <- 16
x1 <- runif(n)
x2 <- runif(n)
group <- as.factor(sample(1:3, n, replace = TRUE))
levels(group) <- c("circle", "triangle", "square")
df <- data.frame(x1, x2, group)
df[1, 2] <- .765 # tweaks to make a more interesting configuration
df[9, 1] <- .741
df <- df[-7, ]
library(ggplot2)
ggplot(df, aes(x = x1, y = x2, col = group, shape = group)) +
  geom_point(size = 4) +
  scale_x_continuous(expand = c(0, 0) , limits = c(0, 1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) +
  scale_color_discrete(guide = FALSE) +
  scale_shape_discrete(guide = FALSE) +
  theme_bw()

```


### 1. Growing the full classification tree

Use the trees package in R to fit a full unpruned tree to this data set, making splits based on the Gini index. You can find the code to do this in the slides from week 8 or in the lab at the end of Chapter 8 in the book. Please plot the resulting tree.

```{r}
summary(df)
```


```{r}
library(tree)
t1 <- tree(group ~ x1 + x2, 
           data = df, split = "gini")
class(t1)
plot(t1)
text(t1, pretty = 0)
set.seed(39)

```

1) The two most common splits that we saw in class were a horizontal split around $X_2 \approx 0.50$ and a vertical split around $X_1 \approx 0.30$. Was either of these the first split decided upon by your classification tree?

No, the tree did not use either of those as the first split. Instead the first split was $X_12 \approx 0.36$. 

2) What is the benefit of the second split in the tree?

I see no benefit to the second split in the tree, since it predicts the same thing either way. 

3) Which class would this model predict for the new observation with $X_1 = 0.21, X_2 = 0.56$?

This model uses none of the infomation from $X_1$ to classify so it would just depend on $X_2$. Given that $X_2 = 0.56 > 0.36$, it will predict a square. 



### 2. An alternate metric

Now refit the tree based on the deviance as the splitting criterion (you set this as an argument to the tree() function). The deviance is defined for the classification setting as:

$$ -2 \sum_m \sum_k n_{mk} \log \hat{p}_{mk} $$

Plot the resulting tree. Why does this tree differ from the tree fit based on the Gini Index?

```{r}
t1 <- tree(group ~ x1 + x2, 
           data = df, split = "deviance")
class(t1)
plot(t1)
text(t1, pretty = 0)
set.seed(39)
```

Yes, this tree differs quite a lot from the previous tree. The previous tree using gini for splitting criteria, only predicted circles or squares and had three leaves. This tree has two leaves and predicts circles and triangles. The split is also a higher value and close to what many groups did in class. The split is different because it is trying to minimize different criteria. 


### 3. Growing a pruned regression tree

Crime and Communities, revisited

In Lab 3, you fit a regression model to a training data set that predicted the crime rate in a community as a function of properties of that community.

Fit a regression tree to the training data using the default splitting criteria (here, the deviance is essentially the RSS). Next, perform cost-complexity pruning and generate a plot showing the relationship between tree size and deviance to demonstrate the size of the best tree. Finally, construct the tree diagram for this best tree.


```{r}
dcrime <- read.csv("http://andrewpbray.github.io/data/crime-train.csv")
treecrime <- tree(ViolentCrimesPerPop ~ racePctWhite 
          + pctUrban  
          + PctEmploy 
          + MalePctDivorce 
          + PctKids2Par 
          + PctWorkMom 
          + PctPersDenseHous 
          + NumStreet 
          + PctVacantBoarded 
          + PctImmigRec8 
          + PctIlleg 
          + PctHousOccup 
          + population 
          + householdsize
          + medIncome, 
           data = dcrime)

plot(treecrime)
text(treecrime, pretty = 0)
set.seed(39)

```

```{r}
set.seed(39)
ptree <- prune.tree(treecrime, k = NULL, best = NULL, dcrime,
           method = c("deviance"), eps = 1e-3)
ptree
```


```{r}
plot(ptree$size, ptree$dev, type = "b")
```

Looks like the tree with size 13 was the best at reducing the deviance.  
```{r}
ptree$size[which.min(ptree$dev)]
ptreebest <- prune.tree(treecrime, k = NULL, best =13, dcrime,
           method = c("deviance"), eps = 1e-3)
plot(ptreebest)
text(ptreebest, pretty = 20, cex = .75, offset = 20)
```



### 4. Comparing predictive performance

Use this tree to compute the MSE for the test data set. How does it compare to the test MSE for your regression model? You can load the test data with the following code:

```{r}
test_data <- read.csv("https://bit.ly/2PYS8Ap")

MSE <- function(model, data){
  n <- nrow(data)
  ys <- data$ViolentCrimesPerPop 
  y_hats <- predict(model, data)
  residuals <- y_hats - ys
  MSE <- sum(residuals^2)/n
  MSE
}

MSEtree <- MSE(ptreebest, test_data)
MSEtree


regmodel <- lm(data = dcrime, ViolentCrimesPerPop ~  
            racePctWhite 
          + pctUrban  
          + PctEmploy 
          + MalePctDivorce 
          + PctKids2Par 
          + PctWorkMom 
          + PctPersDenseHous 
          + NumStreet 
          + PctVacantBoarded 
          + PctImmigRec8 
          + PctIlleg 
          + PctHousOccup
          + population 
          + householdsize
          + medIncome)

MSEreg <- MSE(regmodel, test_data)
MSEreg

```

In this case the MSE for the tree model is better than the MSE for the regression model. This could be due to the fact that the functional form of our data is not well approximated by a linear model. However, the two are not that far apart. 

### 5. Growing a random forest

We now apply methods to decrease the variance of our estimates. Fit a randomForest() model that performs only bagging and no actual random forests (recall that bagging is the special case of random forests with $m = p$). Next, fit a second random forest model that uses $m = p/3$. Compute their test MSE's. Is this an improvement over the vanilla pruned regression tree? Does it beat your regression model?

```{r}
rforest <- randomForest(ViolentCrimesPerPop ~ racePctWhite 
          + pctUrban  
          + PctEmploy 
          + MalePctDivorce 
          + PctKids2Par 
          + PctWorkMom 
          + PctPersDenseHous 
          + NumStreet 
          + PctVacantBoarded 
          + PctImmigRec8 
          + PctIlleg 
          + PctHousOccup
          + population 
          + householdsize
          + medIncome, 
           data = dcrime, mtry = 15)

```

```{r}
rforest
plot(rforest)
```


```{r}
rforest2 <- randomForest(ViolentCrimesPerPop ~ racePctWhite 
          + pctUrban  
          + PctEmploy 
          + MalePctDivorce 
          + PctKids2Par 
          + PctWorkMom 
          + PctPersDenseHous 
          + NumStreet 
          + PctVacantBoarded 
          + PctImmigRec8 
          + PctIlleg 
          + PctHousOccup
          + population 
          + householdsize
          + medIncome, 
           data = dcrime)
```
Since originally we had 15 predictors in the model, the random forest is considering 5 at each split. 
```{r}
rforest2
plot(rforest2)
```

```{r}
MSE(rforest, test_data)
MSE(rforest2, test_data)
```

The test MSE is way better than with the regression model or the pruned tree. It appears that the test MSE for the random forest where m = p is very slightly lower, perhaps if I had included more of the predictors that would not be the case. 


### 6. Variance importance

One thing we lose by using these computational techniques to limit the variance is the clearly interpretable tree diagram. We can still salvage some interpretability by considering importance(). Please construct a Variable Importance Plot (varImpPlot()). Are these restults similar/different from your interpretation of your regression coefficients in Lab 3?

```{r}
varImpPlot(rforest)
```


```{r}
regm <- lm( ViolentCrimesPerPop ~ racePctWhite 
          + pctUrban  
          + PctEmploy 
          + MalePctDivorce 
          + PctKids2Par 
          + PctWorkMom 
          + PctPersDenseHous 
          + NumStreet 
          + PctVacantBoarded 
          + PctImmigRec8 
          + PctIlleg 
          + PctHousOccup
          + population 
          + householdsize
          + medIncome, 
           data = dcrime)
summary(regm)
```

The two appear similar however there are some notable differences. The variables racePctWhite and Pct Illeg are the second and third most important variables in the random forest and similarly are very significant with relatively large coefficients in the regression linear model. The variable PctKids2Par shows up as the most important variable but lacks significance despite having a largeish coefficient in the linear model. Overall, the two clearly correspond to some degree. 

### Two Cultures 

1)What are the two cultures outlined by Breiman?

One culture assumes that data are generated by some stochastic data model (this includes linear and logistic regression type models). The other doesn’t make that kind of assumption about knowing the true form, and rather uses algorithmic models. 

2)Is the Ozone Project supervised or unsupervised? Classification or Regression? Which methods that we’ve seen could be used to tackle this problem?

It was supervised and used regression analysis. Perhaps, binary trees would work better for that data or some form of discriminant analysis. 

3)What is the name of the model/method that is discussed in equation (R) of section 5.1?

That is the assumed functional form for linear regression. 

4)In section 5.4 he states, “If the model has too many parameters, then it may overfit the data and give a biased estimate of accuracy”. Where would this model be in terms of the bias-variance tradeoff?

This would lean towards high variance and low bias since the model is overly complex. 

5)What is the Rashoman effect? Did you run into this effect is question 5 from the last lab?

It is possible to have a large number of models with different parameters that yield about the same RSS. This could also arise with trees when the data used is slightly different. 

6)Explain how one of the techniques that we’ve covered could be seen to invoke Occam’s Razor.

Linear regression is very easy to interpret, however it might not be the best predictor depending on the structure of the data. 

7)The most illuminating point for me in this paper was…

“For a data model, this translates as: fit the parameters in your model by using the data, then, using the model, predict the data and see how good the prediction is.” I hadn’t thought about it like that before as going backward through the model mechanism to generate the data. 

8)The most confusing point for me in this paper was…
 
I thought the section on Bellman and the curse of dimensionality was somewhat confusing. 
 
9)Which of the responses (Cox, Efron, Hoadley, Parzen) do you find the most incisive? Why?
 
I liked Hoadley’s response about how algorithmic models can be highly contextual and only have really good predictive ability on a sample of the data. It seems to me that algorithmic models may be less desirable when data collection was not carried out well (small or biased sample), since no functional form about how the variables should cooperate is assumed. 
 
10)Which do you think is the strongest single criticism of Breiman’s paper that is levelled by the commentators?
 
Cox seems to have the greatest disagreement with the paper and I like his point about how understanding the approximating relationship between variables is important when you want to know about a somewhat related question but have to make use of existing data sources. 
 
11)The big ticket question: in your area of study, if you had to use methods from only one of Breiman’s cultures for the rest of your life, which would it be: Data Model or Algorithmic Model?
 
I’d probably choose the data model because it seems much more likely I’ll use it in the future, although I do agree that algorithmic models are probably generally better since they don’t seem as subject to faulty assumptions of form. 






