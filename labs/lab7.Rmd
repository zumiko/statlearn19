---
title: "Lab 7: When a guest arrives they will count how many sides it has on"
author: "Claire Jellison"
date: "11/8/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown



```{r}
set.seed(75)
n <- 16
x1 <- runif(n)
x2 <- runif(n)
group <- as.factor(sample(1:3, n, replace = TRUE))
levels(group) <- c("circle", "triangle", "square")
df <- data.frame(x1, x2, group)
df[1, 2] <- .765 # tweaks to make a more interesting configuration
df[9, 1] <- .741
df <- df[-7, ]
library(ggplot2)
ggplot(df, aes(x = x1, y = x2, col = group, shape = group)) +
  geom_point(size = 4) +
  scale_x_continuous(expand = c(0, 0) , limits = c(0, 1)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 1)) +
  scale_color_discrete(guide = FALSE) +
  scale_shape_discrete(guide = FALSE) +
  theme_bw()

```


### 1. Growing the full classification tree

Use the trees package in R to fit a full unpruned tree to this data set, making splits based on the Gini index. You can find the code to do this in the slides from week 8 or in the lab at the end of Chapter 8 in the book. Please plot the resulting tree.

```{r}
summary(df)
```


```{r}
library(tree)
t1 <- tree(group ~ x1 + x2, 
           data = df, split = "gini")
class(t1)
plot(t1)
text(t1, pretty = 0)
set.seed(39)

```

1) The two most common splits that we saw in class were a horizontal split around $X_2 \approx 0.50$ and a vertical split around $X_1 \approx 0.30$. Was either of these the first split decided upon by your classification tree?

No, the tree did not use either of those as the first split. Instead the first split was $X_12 \approx 0.36$. 

2) What is the benefit of the second split in the tree?

I see no benefit to the second split in the tree, since it predicts the same thing either way. 

3) Which class would this model predict for the new observation with $X_1 = 0.21, X_2 = 0.56$?

This model uses none of the infomation from $X_1$ to classify so it would just depend on $X_2$. Given that $X_2 = 0.56 > 0.36$, it will predict a square. 



### 2. An alternate metric

Now refit the tree based on the deviance as the splitting criterion (you set this as an argument to the tree() function). The deviance is defined for the classification setting as:

$$ -2 \sum_m \sum_k n_{mk} \log \hat{p}_{mk} $$

Plot the resulting tree. Why does this tree differ from the tree fit based on the Gini Index?

```{r}
t1 <- tree(group ~ x1 + x2, 
           data = df, split = "deviance")
class(t1)
plot(t1)
text(t1, pretty = 0)
set.seed(39)
```

Yes, this tree differs quite a lot from the previous tree. The previous tree using gini for splitting criteria, only predicted circles or squares and had three leaves. This tree has two leaves and predicts circles and triangles. The split is also a higher value and close to what many groups did in class. The split is different because it is trying to minimize different criteria. 



### 3. Growing a pruned regression tree

Crime and Communities, revisited

In Lab 3, you fit a regression model to a training data set that predicted the crime rate in a community as a function of properties of that community.

Fit a regression tree to the training data using the default splitting criteria (here, the deviance is essentially the RSS). Next, perform cost-complexity pruning and generate a plot showing the relationship between tree size and deviance to demonstrate the size of the best tree. Finally, construct the tree diagram for this best tree.


```{r}
dcrime <- read.csv("http://andrewpbray.github.io/data/crime-train.csv")
treecrime <- tree(ViolentCrimesPerPop ~ racePctWhite 
          + pctUrban  
          + PctEmploy 
          + MalePctDivorce 
          + PctKids2Par 
          + PctWorkMom 
          + PctPersDenseHous 
          + NumStreet 
          + PctVacantBoarded 
          + PctImmigRec8 
          + PctIlleg 
          + PctHousOccup, 
           data = dcrime)

plot(treecrime)
text(treecrime, pretty = 0)
set.seed(39)

```

```{r}
set.seed(39)
ptree <- prune.tree(treecrime, k = NULL, best = NULL, dcrime,
           method = c("deviance"), eps = 1e-3)
ptree
```


```{r}
plot(ptree$size, ptree$dev, type = "b")
```

Looks like the largest tree was the best at reducing the deviance. 
```{r}
ptree$size[which.min(ptree$dev)]
ptreebest <- prune.tree(treecrime, k = NULL, best =13, dcrime,
           method = c("deviance"), eps = 1e-3)
plot(ptreebest)
text(ptreebest, pretty = 20, cex = .75, offset = 20)
```



### 4. Comparing predictive performance

Use this tree to compute the MSE for the test data set. How does it compare to the test MSE for your regression model? You can load the test data with the following code:

```{r}
test_data <- read.csv("https://bit.ly/2PYS8Ap")

MSE <- function(model, data){
  n <- nrow(data)
  ys <- data$ViolentCrimesPerPop 
  y_hats <- predict(model, data)
  residuals <- y_hats - ys
  MSE <- sum(residuals^2)/n
  MSE
}

MSEtree <- MSE(ptreebest, test_data)
MSEtree


regmodel <- lm(data = dcrime, ViolentCrimesPerPop ~  
            racePctWhite 
          + pctUrban  
          + PctEmploy 
          + MalePctDivorce 
          + PctKids2Par 
          + PctWorkMom 
          + PctPersDenseHous 
          + NumStreet 
          + PctVacantBoarded 
          + PctImmigRec8 
          + PctIlleg 
          + PctHousOccup)

MSEreg <- MSE(regmodel, test_data)
MSEreg

```

In this case the MSE for the tree model is better than the MSE for the regression model. This could be due to the fact that the functional form of our data is not well approximated by a linear model.

### 5. Growing a random forest

We now apply methods to decrease the variance of our estimates. Fit a randomForest() model that performs only bagging and no actual random forests (recall that bagging is the special case of random forests with (m = p)). Next, fit a second random forest model that uses (m = p/3). Compute their test MSEs. Is this an improvement over the vanilla pruned regression tree? Does it beat your regression model?

```{r}

```


### 6. Variance importance

One thing we lose by using these computational techniques to limit the variance is the clearly interpretable tree diagram. We can still salvage some interpretability by considering importance(). Please construct a Variable Importance Plot (varImpPlot()). Are these restults similar/different from your interpretation of your regression coefficients in Lab 3?

```{r}

```





